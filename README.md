### V01:
- paper: multiway attention networks for modeling sentences pairs
- add qanet attention(trilinear attenion)
- add slqa attention (bilinear attention)
- using softmax to give each attention a weight
- add char embedding and alignment with word embedding


### V02:
- QANet

### V03:
- DGCNN, encoder with dilate convolution

### V04:
- multi-cast attention 

BERT:
- exture chinese character embedding from offical tensorflow checkpoint
